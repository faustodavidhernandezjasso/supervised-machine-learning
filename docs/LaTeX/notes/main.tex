\documentclass[a4paper]{article} 
\input{head}
\newcommand{\pow}[2]{#1^{#2}}
\newcommand{\supra}[1]{\textsuperscript{#1}}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Fausto David Hern√°ndez Jasso \hfill\\   
317000928 \hfill\\
fausto.david.hernandez.jasso@ciencias.unam.mx
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Notes\\ 
\normalsize 
Supervised Machine Learning: Regression and Classification\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip
\section{Introduction}
\subsection{What is machine learning?}
\noindent
According to \textbf{Arthur Samuel} is: "Field of study that gives computers
the ability to learn without being explicitly programmed."
\subsection{Classification of machine learning algorithms}
\noindent
\begin{itemize}
    \item Supervised learning.
    \item Unsupervised learning.
    \item Recommender systems.
    \item Reinforcement learning.
\end{itemize}
\subsection{Supervised Learning}
\noindent
It refers to algorithms that learn \(x\) to \(y\) or input to output mappings.
The key characteristic of supervised learning is that you give your learning 
algorithm examples to learn from that includes the \textit{right answers}.
This it means the correct label \(y\) for a given input \(x\).
\subsubsection{Housing price prediction}
\noindent
We want to predict housing prices based on the size of the house.
\subsubsection{Regression}
\noindent
In this type of supervised learning we are trying to predict a number from infinitely 
many possible numbers.
\subsubsection{Classification}
\noindent
This kind of algorithm predicts categories. Categories don't have to be numbers.
\subsection{Unsupervised Learning}
\noindent
In these kind of algorithms were given data that isn't associated with any output
lables \(y\). Our goal is finding something interesting in unlabeled data. We are not trying to
supervised the algorithm to give some quote right answer for every input, instead we asked the 
algorithm to figure out all by itself. 
\subsubsection{Formal definition}
\noindent
Data only comes with inputs \(x\), but not output labels \(y\). Algorithm has to find a structure.
\subsubsection{Clustering algorithms}
\noindent
It takes data without label and tries to automatically group them into clusters.
\subsubsection{Anomaly detection}
\noindent
Find unusual effects.
\subsubsection{Dimensionaly reduction}
\noindent
This algorithm let you take a big data set and compress it to a much smaller data set lossing 
little information as possible.
\section{Linear Regression}
\noindent
This model just means fitting a straigh line to your data. It is called regression model because 
it predicts numbers as the output. There are infinitely many possible numbers that the model could
output.
\subsection{Terminology}
\subsubsection{Training set}
\noindent
Data used to train the model.
\subsection{Notation}
\noindent
\begin{itemize}
    \item \(x \ = \ \) input variable.
    \item \(y \ = \ \) output or target variable. 
    \item \(m \ = \ \) number of training examples.
    \item \((x, y)\) single training example.
    \item \((x^{(i)}, y^{(i)})\) \(i^{\text{th}}\) training example with \(1 \leq i \leq m\). 
\end{itemize}
For representing the learning function we use
\[
    f_{w,b}\left(x\right) = wx + b  
\]
\subsection{Linear Regression with one variable or univariate linear regression}
\noindent
Is a single input variable or feature \(x\).
\subsection{Cost function}
\noindent
It will tell us how well the model is doing. We have a training set that contains input 
features \(x\) and output targets \(y\). The model we are going to use to fit this training set
is this linear function:
\[
    f_{w,b}\left(x\right) = wx + b  
\]
\(w\) and \(b\) are called parameters of the model.
\newline 
\(b\) is also called the \(y\) intercep because crossed the vertical axis. The value of \(w\) give us 
the slope of the line. In linear regression we want to do is to choose values for \(w\) and \(b\) so
that straight line you get from the function \(f\) somehow fits the data well.
\[
    \hat{y} = f_{w,b}\left(x^{(i)}\right)  
\]
The \textbf{cost function} takes the prediction \(\hat{y}\) and compares it to the target \(y\) by
taking \(\hat{y} - y\) this called the error.
\newline
The final \textbf{cost function} is:
\[
    J\left(w, b\right) = \frac{1}{m} \sum_{i = 1}^{m}\left(\hat{y}^{(i)} - y^{(i)}\right)^2  
\]
this \textbf{cost function} is called squared error.
\subsubsection{Recap}
\begin{itemize}
    \item model: \(f_{w,b} = wx + b\)
    \item parameters: \(w,b\)
    \item cost function: \(J\left(w, b\right) = \frac{1}{m}\sum_{i = 1}^{m}\left(f_{w,b}\left(x^{(i)}\right) - y^{(i)}\right)^2\)
    \item goal: \(\displaystyle \min_{w,b} J\left(w,b\right)\)
\end{itemize}
\end{document}